{
 "cells": [
  {
   "cell_type": "raw",
   "id": "ac4b8660",
   "metadata": {},
   "source": [
    "https://lightning.ai/docs/pytorch/stable/data/datamodule.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af7092d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch import optim, nn, utils, Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.transforms as transforms\n",
    "import lightning.pytorch as pl\n",
    "\n",
    "from lightning.pytorch.tuner import Tuner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "923102ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir: str = \"path/to/dir\", batch_size: int = 32):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def setup(self, stage: str):\n",
    "        self.mnist_test = MNIST(self.data_dir, train=False)\n",
    "        self.mnist_predict = MNIST(self.data_dir, train=False)\n",
    "        mnist_full = MNIST(self.data_dir, train=True)\n",
    "        self.mnist_train, self.mnist_val = utils.data.random_split(mnist_full, [55000, 5000])\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.mnist_train, batch_size=self.batch_size)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.mnist_val, batch_size=self.batch_size)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.mnist_test, batch_size=self.batch_size)\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(self.mnist_predict, batch_size=self.batch_size)\n",
    "\n",
    "    def teardown(self, stage: str):\n",
    "        # Used to clean-up when the run is finished\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a66ccdca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_path:  ..\\..\\datasets\n"
     ]
    }
   ],
   "source": [
    "# setup data\n",
    "my_path = os.path.join(\"..\", \"..\", \"datasets\")\n",
    "print('my_path: ', my_path)\n",
    "\n",
    "mnist = MNISTDataModule(my_path)\n",
    "# model = LitClassifier()\n",
    "\n",
    "# trainer = Trainer()\n",
    "# trainer.fit(model, mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bada287",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir: str = \"./\"):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # download\n",
    "        MNIST(self.data_dir, train=True, download=True)\n",
    "        MNIST(self.data_dir, train=False, download=True)\n",
    "        \n",
    "        # For eg., if you are working with NLP task where you need to tokenize the text and use it,\n",
    "        # then you can do something like as follows:\n",
    "        # tokenize\n",
    "        # save it to disk\n",
    "\n",
    "    def setup(self, stage: str):\n",
    "        # Assign train/val datasets for use in dataloaders\n",
    "        if stage == \"fit\":\n",
    "            mnist_full = MNIST(self.data_dir, train=True, transform=self.transform)\n",
    "            self.mnist_train, self.mnist_val = utils.data.random_split(mnist_full, [55000, 5000])\n",
    "\n",
    "        # Assign test dataset for use in dataloader(s)\n",
    "        if stage == \"test\":\n",
    "            self.mnist_test = MNIST(self.data_dir, train=False, transform=self.transform)\n",
    "\n",
    "        if stage == \"predict\":\n",
    "            self.mnist_predict = MNIST(self.data_dir, train=False, transform=self.transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.mnist_train, batch_size=32)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.mnist_val, batch_size=32)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.mnist_test, batch_size=32)\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(self.mnist_predict, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df0d21cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = MNISTDataModule(my_path)\n",
    "# model = Model()\n",
    "# trainer.fit(model, datamodule=dm)\n",
    "# trainer.test(datamodule=dm)\n",
    "# trainer.validate(datamodule=dm)\n",
    "# trainer.predict(datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d62e0225",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = MNISTDataModule(my_path)\n",
    "dm.prepare_data()\n",
    "dm.setup(stage=\"fit\")\n",
    "\n",
    "# model = Model(num_classes=dm.num_classes, width=dm.width, vocab=dm.vocab)\n",
    "# trainer.fit(model, dm)\n",
    "\n",
    "# dm.setup(stage=\"test\")\n",
    "# trainer.test(datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb4bbbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
