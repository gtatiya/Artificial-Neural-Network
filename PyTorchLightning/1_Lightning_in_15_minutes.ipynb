{
 "cells": [
  {
   "cell_type": "raw",
   "id": "5cbe7bc7",
   "metadata": {},
   "source": [
    "https://lightning.ai/docs/pytorch/stable/starter/introduction.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da4436ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch import optim, nn, utils, Tensor\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "import lightning.pytorch as pl\n",
    "\n",
    "from lightning.pytorch.tuner import Tuner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad19dcc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASETS_PATH:  ..\\..\\datasets\n"
     ]
    }
   ],
   "source": [
    "# setup data\n",
    "DATASETS_PATH = os.path.join(\"..\", \"..\", \"datasets\")\n",
    "print('DATASETS_PATH: ', DATASETS_PATH)\n",
    "\n",
    "dataset = MNIST(DATASETS_PATH, download=True, transform=ToTensor())\n",
    "train_loader = utils.data.DataLoader(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0210382b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# define any number of nn.Modules (or use your current ones)\n",
    "encoder = nn.Sequential(nn.Linear(28 * 28, 64), nn.ReLU(), nn.Linear(64, 3))\n",
    "decoder = nn.Sequential(nn.Linear(3, 64), nn.ReLU(), nn.Linear(64, 28 * 28))\n",
    "\n",
    "\n",
    "# define the LightningModule\n",
    "class LitAutoEncoder(pl.LightningModule):\n",
    "    def __init__(self, encoder, decoder, batch_size=32, train_loader=None):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.batch_size = batch_size\n",
    "        self.train_loader = train_loader\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        # it is independent of forward\n",
    "        x, y = batch\n",
    "        x = x.view(x.size(0), -1)\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        loss = nn.functional.mse_loss(x_hat, x)\n",
    "        # Logging to TensorBoard (if installed) by default\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return self.train_loader\n",
    "\n",
    "\n",
    "# init the autoencoder\n",
    "batch_size = 32\n",
    "autoencoder = LitAutoEncoder(encoder, decoder, batch_size, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81d78a80",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autoencoder.batch_size:  32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missing logger folder: C:\\Users\\GyanT\\Documents\\GitHub\\Artificial-Neural-Network\\PyTorchLightning\\lightning_logs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\GyanT\\miniconda3\\envs\\py3_10\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 2 succeeded, trying batch size 4\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 4 succeeded, trying batch size 8\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 8 succeeded, trying batch size 16\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 16 succeeded, trying batch size 32\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 32 succeeded, trying batch size 64\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 64 succeeded, trying batch size 128\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 128 succeeded, trying batch size 256\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 256 succeeded, trying batch size 512\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 512 succeeded, trying batch size 1024\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 1024 succeeded, trying batch size 2048\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 2048 succeeded, trying batch size 4096\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 4096 succeeded, trying batch size 8192\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 8192 succeeded, trying batch size 16384\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 16384 succeeded, trying batch size 32768\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "Batch size 32768 succeeded, trying batch size 65536\n",
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "The batch size 65536 is greater or equal than the length of your dataset.\n",
      "Finished batch size finder, will continue with full run using batch size 65536\n",
      "Restoring states from the checkpoint path at C:\\Users\\GyanT\\Documents\\GitHub\\Artificial-Neural-Network\\PyTorchLightning\\.scale_batch_size_d9420eab-829b-42aa-8926-6959f0438f07.ckpt\n",
      "Restored all states from the checkpoint at C:\\Users\\GyanT\\Documents\\GitHub\\Artificial-Neural-Network\\PyTorchLightning\\.scale_batch_size_d9420eab-829b-42aa-8926-6959f0438f07.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "The batch size 65536 is greater or equal than the length of your dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autoencoder.batch_size:  65536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=3` reached.\n",
      "The batch size 65536 is greater or equal than the length of your dataset.\n",
      "Finished batch size finder, will continue with full run using batch size 65536\n",
      "Restoring states from the checkpoint path at C:\\Users\\GyanT\\Documents\\GitHub\\Artificial-Neural-Network\\PyTorchLightning\\.scale_batch_size_6c8e5620-87db-4696-9568-5ab58b5c5dfe.ckpt\n",
      "Restored all states from the checkpoint at C:\\Users\\GyanT\\Documents\\GitHub\\Artificial-Neural-Network\\PyTorchLightning\\.scale_batch_size_6c8e5620-87db-4696-9568-5ab58b5c5dfe.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type       | Params\n",
      "---------------------------------------\n",
      "0 | encoder | Sequential | 50.4 K\n",
      "1 | decoder | Sequential | 51.2 K\n",
      "---------------------------------------\n",
      "101 K     Trainable params\n",
      "0         Non-trainable params\n",
      "101 K     Total params\n",
      "0.407     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autoencoder.batch_size:  65536\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28da7682b31f4e028113c38178028014",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    }
   ],
   "source": [
    "# train the model (hint: here are some helpful Trainer arguments for rapid idea iteration)\n",
    "# https://lightning.ai/docs/pytorch/stable/common/trainer.html\n",
    "# https://lightning.ai/docs/pytorch/stable/advanced/training_tricks.html\n",
    "\n",
    "trainer = pl.Trainer(limit_train_batches=100, max_epochs=1)\n",
    "# print('trainer: ', help(trainer))\n",
    "\n",
    "# Create a tuner for the trainer\n",
    "tuner = Tuner(trainer)\n",
    "print('autoencoder.batch_size: ', autoencoder.batch_size)\n",
    "\n",
    "# Auto-scale batch size by growing it exponentially (default)\n",
    "tuner.scale_batch_size(model=autoencoder, mode=\"power\")\n",
    "print('autoencoder.batch_size: ', autoencoder.batch_size)\n",
    "\n",
    "# Auto-scale batch size with binary search\n",
    "tuner.scale_batch_size(model=autoencoder, mode=\"binsearch\")\n",
    "print('autoencoder.batch_size: ', autoencoder.batch_size)\n",
    "\n",
    "trainer.fit(model=autoencoder, train_dataloaders=train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ec2e9f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡ \n",
      "Predictions (4 image embeddings):\n",
      " tensor([[0.0088, 0.1069, 0.0755],\n",
      "        [0.0088, 0.1069, 0.0755],\n",
      "        [0.0088, 0.1069, 0.0755],\n",
      "        [0.0088, 0.1069, 0.0755]], device='cuda:0', grad_fn=<AddmmBackward0>) \n",
      " ⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡⚡\n"
     ]
    }
   ],
   "source": [
    "# load checkpoint\n",
    "checkpoint = \"./lightning_logs/version_0/checkpoints/epoch=0-step=100.ckpt\"\n",
    "autoencoder = LitAutoEncoder.load_from_checkpoint(checkpoint, encoder=encoder, decoder=decoder)\n",
    "\n",
    "# choose your trained nn.Module\n",
    "encoder = autoencoder.encoder\n",
    "encoder.eval()\n",
    "\n",
    "# embed 4 fake images!\n",
    "fake_image_batch = Tensor(4, 28 * 28).to(device='cuda')\n",
    "embeddings = encoder(fake_image_batch)\n",
    "print(\"⚡\" * 20, \"\\nPredictions (4 image embeddings):\\n\", embeddings, \"\\n\", \"⚡\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d32366",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
