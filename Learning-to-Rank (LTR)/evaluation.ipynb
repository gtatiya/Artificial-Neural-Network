{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66a41111-ff27-4ed1-b9eb-1719ba5f7383",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e184d231-f28f-4556-9a37-ace1f4aba850",
   "metadata": {},
   "source": [
    "# Mean reciprocal rank (MRR)\n",
    "\n",
    "\\begin{align}\n",
    "MRR = \\frac{1}{|Q|} \\sum_{i=1}^{|Q|} \\frac{1}{rank_i}\n",
    "\\end{align}\n",
    "Where: <br>\n",
    "|Q| is the number of queries or user lists <br>\n",
    "rank_i is the position of the first relevant item for the i-th query\n",
    "\n",
    "GT:\n",
    "- The more the relevant item will be in the top ranked items, the more MRR value will be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3668c170-aecb-4af0-9a0d-efe909512cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reciprocal Rank: 0.4583333333333333\n"
     ]
    }
   ],
   "source": [
    "def mean_reciprocal_rank(relevance_lists):\n",
    "    \"\"\"\n",
    "    Calculate Mean Reciprocal Rank (MRR)\n",
    "    \n",
    "    Args:\n",
    "    relevance_lists (list of lists): Each inner list represents relevance scores for a query,\n",
    "                                     where 1 indicates a relevant item and 0 an irrelevant item.\n",
    "    \n",
    "    Returns:\n",
    "    float: The Mean Reciprocal Rank\n",
    "    \"\"\"\n",
    "    \n",
    "    reciprocal_ranks = []\n",
    "    for relevance in relevance_lists:\n",
    "        try:\n",
    "            # Find the index of the first relevant item (1)\n",
    "            rank = relevance.index(1) + 1  # +1 because index is 0-based\n",
    "            reciprocal_ranks.append(1 / rank)\n",
    "        except ValueError:\n",
    "            # If no relevant item is found, use 0\n",
    "            reciprocal_ranks.append(0)\n",
    "    \n",
    "    return np.mean(reciprocal_ranks)\n",
    "\n",
    "# Example usage\n",
    "relevance_lists = [\n",
    "    [0, 0, 1, 0, 0],  # First relevant item at position 3\n",
    "    [1, 0, 0, 0, 0],  # First relevant item at position 1\n",
    "    [0, 0, 0, 0, 0],  # No relevant item\n",
    "    [0, 1, 0, 0, 0]   # First relevant item at position 2\n",
    "]\n",
    "\n",
    "mrr = mean_reciprocal_rank(relevance_lists)\n",
    "print(f\"Mean Reciprocal Rank: {mrr}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "95f60cf9-cded-48b3-baa2-d2f3b8df31cf",
   "metadata": {},
   "source": [
    "# Mean Average Precision (MAP)\n",
    "\n",
    "\\begin{align}\n",
    "MAP = \\frac{1}{|Q|} \\sum_{q=1}^{|Q|} AP(q)\n",
    "\\end{align}\n",
    "Where:\n",
    "|Q| is the number of queries\n",
    "AP(q) is the Average Precision for a single query q\n",
    "The Average Precision (AP) for a single query is calculated as:\n",
    "\\begin{align}\n",
    "AP = \\sum_{k=1}^n P(k) \\times rel(k)\n",
    "\\end{align}\n",
    "Where:\n",
    "k is the rank in the sequence of retrieved items\n",
    "n is the number of retrieved items\n",
    "P(k) is the precision at cut-off k in the list\n",
    "rel(k) is an indicator function equaling 1 if the item at rank k is relevant, zero otherwise\n",
    "\n",
    "GT:\n",
    "- The higher the relevance score of the top ranked items are, the more MAP value will be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4f8e319-323f-4d7f-bcaf-403b2174509f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sorted_indices: [0 3 2 1 4], y_true: [1 1 1 0 0], [1 2 3 3 3], [1 2 3 4 5], [1.   1.   1.   0.75 0.6 ]\n",
      "sorted_indices: [2 4 1 3 0], y_true: [1 0 1 0 0], [1 1 2 2 2], [1 2 3 4 5], [1.   0.5  0.67 0.5  0.4 ]\n",
      "Mean Average Precision: 0.9166666666666666\n"
     ]
    }
   ],
   "source": [
    "def average_precision(y_true, y_scores):\n",
    "    \"\"\"Calculate average precision for a single query\"\"\"\n",
    "    sorted_indices = np.argsort(y_scores)[::-1]\n",
    "    y_true = np.take(y_true, sorted_indices)\n",
    "    \n",
    "    precisions = np.cumsum(y_true) / (np.arange(len(y_true)) + 1)\n",
    "    print(f'sorted_indices: {sorted_indices}, y_true: {y_true}, {np.cumsum(y_true)}, {np.arange(len(y_true)) + 1}, {precisions.round(2)}')\n",
    "    return np.sum(precisions * y_true) / np.sum(y_true)\n",
    "\n",
    "def mean_average_precision(y_true, y_scores):\n",
    "    \"\"\"Calculate Mean Average Precision for multiple queries\"\"\"\n",
    "    aps = [average_precision(y_true[i], y_scores[i]) for i in range(len(y_true))]\n",
    "    return np.mean(aps)\n",
    "\n",
    "# Example usage\n",
    "y_true = [\n",
    "    [1, 0, 1, 1, 0],  # Relevance for query 1\n",
    "    [0, 1, 1, 0, 0]   # Relevance for query 2\n",
    "]\n",
    "y_scores = [\n",
    "    [0.9, 0.2, 0.7, 0.8, 0.1],  # Prediction scores for query 1\n",
    "    [0.1, 0.8, 0.9, 0.3, 0.8]   # Prediction scores for query 2\n",
    "]\n",
    "\n",
    "map_score = mean_average_precision(y_true, y_scores)\n",
    "print(f\"Mean Average Precision: {map_score}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c0807fe6-6ec4-4242-b314-055ea6d16629",
   "metadata": {},
   "source": [
    "# Normalized Discounted Cumulative Gain (NDCG)\n",
    "\n",
    "Normalized Discounted Cumulative Gain (NDCG) measures how well a ranking algorithm orders items based on their relevance, with a focus on the top-ranked items.\n",
    "\n",
    "\\begin{align}\n",
    "NDCG_p = \\frac{DCG_p}{IDCG_p}\n",
    "\\end{align}\n",
    "Where:\n",
    "\\begin{align}\n",
    "DCG_p = \\sum_{i=1}^p \\frac{2^{rel_i} - 1}{\\log_2(i + 1)}\n",
    "\\end{align}\n",
    "And:\n",
    "\\begin{align}\n",
    "IDCG_p = \\sum_{i=1}^{|REL_p|} \\frac{2^{rel_i} - 1}{\\log_2(i + 1)}\n",
    "\\end{align}\n",
    "In these formulas:\n",
    "$p$ is the position up to which NDCG is calculated\n",
    "$rel_i$ is the relevance score of the item at position $i$\n",
    "$|REL_p|$ is the list of relevant documents (ordered by their relevance) up to position $p$\n",
    "\n",
    "GT:\n",
    "- The higher the relevance score of the top ranked items are, the more DCG value will be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b57bd9a-0452-4f17-9910-67e0d0103a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r: [3. 2. 2. 2. 1.], [8. 4. 4. 4. 2.], [7. 3. 3. 3. 1.], [1.   1.58 2.   2.32 2.58],         [7.   1.89 1.5  1.29 0.39],         12.07\n",
      "r: [2. 2. 3. 0. 1.], [4. 4. 8. 1. 2.], [3. 3. 7. 0. 1.], [1.   1.58 2.   2.32 2.58],         [3.   1.89 3.5  0.   0.39],         8.78\n",
      "NDCG@5: 0.7272929761069984\n"
     ]
    }
   ],
   "source": [
    "def dcg_at_k(r, k):\n",
    "    r = np.asfarray(r)[:k]\n",
    "    print(f'r: {r}, {np.power(2, r)}, {np.subtract(np.power(2, r), 1)}, {np.log2(np.arange(2, r.size + 2)).round(2)}, \\\n",
    "        {(np.subtract(np.power(2, r), 1) / np.log2(np.arange(2, r.size + 2))).round(2)}, \\\n",
    "        {np.sum(np.subtract(np.power(2, r), 1) / np.log2(np.arange(2, r.size + 2))).round(2)}')\n",
    "    if r.size:\n",
    "        return np.sum(np.subtract(np.power(2, r), 1) / np.log2(np.arange(2, r.size + 2)))\n",
    "    return 0.\n",
    "\n",
    "def ndcg_at_k(r, k):\n",
    "    dcg_max = dcg_at_k(sorted(r, reverse=True), k)\n",
    "    if not dcg_max:\n",
    "        return 0.\n",
    "    return dcg_at_k(r, k) / dcg_max\n",
    "\n",
    "# Example usage\n",
    "# relevance_scores = [3, 2, 3, 0, 1, 2] # 0.8755\n",
    "# relevance_scores = [4, 2, 3, 0, 1, 2] # 0.9196\n",
    "relevance_scores = [2, 2, 3, 0, 1, 2] # 0.7272\n",
    "k = 5\n",
    "ndcg = ndcg_at_k(relevance_scores, k)\n",
    "print(f\"NDCG@{k}: {ndcg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d53bade-36e6-4eda-bfc5-38ebf31b0ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r: [3. 3. 2. 2. 0.], [8. 8. 4. 4. 1.], [7. 7. 3. 3. 0.], [1.   1.58 2.   2.32 2.58],         [7.   4.42 1.5  1.29 0.  ],         14.21\n",
      "r: [3. 3. 2. 2. 1.], [8. 8. 4. 4. 2.], [7. 7. 3. 3. 1.], [1.   1.58 2.   2.32 2.58],         [7.   4.42 1.5  1.29 0.39],         14.6\n",
      "NDCG@5: 0.973494864667227\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "In the Python implementation of NDCG above, we used relevance_scores for evaluation.\n",
    "Let's say that this relevance_scores came from a method we want to evaluate.\n",
    "How can we use ground truth score for evaluation using NDCG?\n",
    "'''\n",
    "def ndcg_at_k(ground_truth, predicted_order, k):\n",
    "    # Sort ground truth scores based on predicted order\n",
    "    reordered_ground_truth = [ground_truth[i] for i in predicted_order]\n",
    "    \n",
    "    # Calculate DCG of the reordered ground truth\n",
    "    dcg = dcg_at_k(reordered_ground_truth, k)\n",
    "    \n",
    "    # Calculate IDCG (using original ground truth, sorted in descending order)\n",
    "    idcg = dcg_at_k(sorted(ground_truth, reverse=True), k)\n",
    "    \n",
    "    if idcg == 0:\n",
    "        return 0.\n",
    "    \n",
    "    return dcg / idcg\n",
    "\n",
    "# Example usage\n",
    "ground_truth_scores = [3, 2, 3, 0, 1, 2]  # Ground truth relevance scores\n",
    "predicted_order = [0, 2, 1, 5, 3, 4]  # Predicted ranking (as indices of ground_truth_scores)\n",
    "k = 5\n",
    "\n",
    "ndcg = ndcg_at_k(ground_truth_scores, predicted_order, k)\n",
    "print(f\"NDCG@{k}: {ndcg}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "135d5b1f-cc73-4f36-a5c2-f134444944b4",
   "metadata": {},
   "source": [
    "# Rank correlation \n",
    "\n",
    "Rank correlation is a measure of the relationship between the rankings of two variables.\n",
    "It assesses how well the relationship between two variables can be described using a monotonic function, without making any assumptions about the frequency distribution of the variables.\n",
    "Two of the most common rank correlation coefficients are:\n",
    "1. Spearman's rank correlation coefficient (ρ or rs)\n",
    "2. Kendall's tau (τ)\n",
    "\n",
    "The formula for **Spearman's rank correlation coefficient** is:\n",
    "\\begin{align}\n",
    "\\rho = 1 - \\frac{6 \\sum d_i^2}{n(n^2 - 1)}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0340f73-02ce-469b-8e14-9bd210655e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_ranks: [2.5 1.  2.5 4.  5. ], [-1.5  1.   0.5  0.   0. ]\n",
      "Spearman's rank correlation coefficient: 0.825\n",
      "SciPy's Spearman correlation: 0.8207826816681233, pvalue: 0.08858700531354381\n"
     ]
    }
   ],
   "source": [
    "def spearman_rank_correlation(x, y):\n",
    "    # Convert to numpy arrays\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    # Get the ranks\n",
    "    x_ranks = stats.rankdata(x)\n",
    "    y_ranks = stats.rankdata(y)\n",
    "    \n",
    "    # Calculate the difference in ranks\n",
    "    d = x_ranks - y_ranks\n",
    "    print(f'y_ranks: {y_ranks}, {d}')\n",
    "    \n",
    "    # Calculate n\n",
    "    n = len(x)\n",
    "    \n",
    "    # Calculate rho\n",
    "    rho = 1 - (6 * np.sum(d**2)) / (n * (n**2 - 1))\n",
    "    \n",
    "    return rho\n",
    "\n",
    "# Example usage\n",
    "x = [1, 2, 3, 4, 5]\n",
    "y = [2, 1, 2, 4, 5]\n",
    "\n",
    "correlation = spearman_rank_correlation(x, y)\n",
    "print(f\"Spearman's rank correlation coefficient: {correlation}\")\n",
    "\n",
    "# Verify with scipy's implementation\n",
    "scipy_correlation, pvalue = stats.spearmanr(x, y)\n",
    "print(f\"SciPy's Spearman correlation: {scipy_correlation}, pvalue: {pvalue}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae07593-b6a3-48c5-b7fa-e1169d775195",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
